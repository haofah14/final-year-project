{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10960916,"sourceType":"datasetVersion","datasetId":6819154},{"sourceId":279073,"sourceType":"modelInstanceVersion","modelInstanceId":239058,"modelId":260722}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wikipedia spacy --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport zipfile\nimport random\nimport unicodedata\nfrom datasets import load_dataset, Dataset as HFDataset\nimport logging\nfrom tqdm.auto import tqdm\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling,\n    AutoModelForSequenceClassification,\n    DataCollatorForSeq2Seq,\n    GenerationConfig\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import Counter\nfrom collections import defaultdict\nfrom sentence_transformers import SentenceTransformer, util\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport wikipedia","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger()\nlogger.handlers[0].setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('punkt', quiet=True)\nnltk.download('stopwords', quiet=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATASET COLLECTION AND CURATION","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"Improved text cleaning function.\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return \"\"\n\n    text = unicodedata.normalize(\"NFKC\", text)\n    text = re.sub(r\"\\s*\\n+\\s*\", \" \", text)\n    text = re.sub(r\"@+\", \"\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n    text = re.sub(r\"\\s+([?.!,])\", r\"\\1\", text)\n    text = re.sub(r\"([?.!,])\\s+\", r\"\\1 \", text)\n    text = re.sub(r\"(\\w+) '(\\w+)\", r\"\\1'\\2\", text)\n    text = re.sub(r'\\\\\"', '\"', text)\n    text = re.sub(r\"<.*?>\", \"\", text)\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    text = re.sub(r\"http\\S+|www\\S+|ftp\\S+\", \"\", text)\n\n    return text\n\ndef filter_by_length(text, min_length=50, max_length=5000):\n    \"\"\"Filter texts by length to remove very short or very long entries.\"\"\"\n    return min_length <= len(text) <= max_length\n    text = re.sub(r\"<.*?","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collect_diverse_datasets():\n    \"\"\"Collect and combine diverse datasets from Hugging Face with sample limits.\"\"\"\n    datasets = {}\n\n    # 1. News Articles (CNN/DailyMail)\n    print(\"Loading news articles dataset (CNN/DailyMail)...\")\n    news = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n\n    news_df = pd.DataFrame({\n        'text': news['article'],\n        'source': ['news'] * len(news)\n    })\n    datasets['news'] = news_df\n    print(f\"Loaded {len(news_df)} news articles\")\n\n    # 2. Blog Posts (WikiText)\n    print(\"Loading blog-like content (WikiText)...\")\n    wiki = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")\n    wiki_texts = [text for text in wiki['text'] if len(text) > 200 and not text.startswith('=')]\n\n    wiki_df = pd.DataFrame({\n        'text': wiki_texts,\n        'source': ['blog'] * len(wiki_texts)\n    })\n    datasets['blogs'] = wiki_df\n    print(f\"Loaded {len(wiki_df)} blog-like entries from WikiText\")\n\n    # 3. Product Descriptions (Yelp Reviews)\n    print(\"Loading product-related content (Yelp Reviews)...\")\n    yelp = load_dataset(\"yelp_review_full\", split=\"train\")\n\n    yelp_df = pd.DataFrame({\n        'text': yelp['text'],\n        'source': ['product'] * len(yelp)\n    })\n    datasets['products'] = yelp_df\n    print(f\"Loaded {len(yelp_df)} product/business reviews\")\n\n    # 4. Academic Content (GLUE QNLI)\n    print(\"Loading academic dataset (GLUE QNLI)...\")\n    qnli = load_dataset(\"glue\", \"qnli\", split=\"train\")\n\n    qnli_df = pd.DataFrame({\n        'text': qnli['sentence'],\n        'source': ['academic'] * len(qnli)\n    })\n    datasets['academic'] = qnli_df\n    print(f\"Loaded {len(qnli_df)} academic sentences from QNLI\")\n\n    # 5. Books (BookCorpus)\n    print(\"Loading book dataset (BookCorpus)...\")\n    BOOK_SAMPLE_SIZE = 500000\n    books = load_dataset(\"bookcorpus\", split=\"train\", trust_remote_code=True)\n    \n    if len(books) > BOOK_SAMPLE_SIZE:\n        book_texts = books.shuffle(seed=42).select(range(BOOK_SAMPLE_SIZE))['text']\n\n    book_df = pd.DataFrame({'text': book_texts, 'source': ['books'] * len(book_texts)})\n    datasets['books'] = book_df\n    print(f\"Loaded {len(book_df)} book passages from BookCorpus (Sampled)\")\n    \n    return datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def curate_dataset(datasets):\n    \"\"\"Clean and prepare the collected datasets.\"\"\"\n    print(\"Combining datasets...\")\n    combined_df = pd.concat(datasets.values(), ignore_index=True)\n\n    print(f\"Raw combined dataset size: {len(combined_df)} documents\")\n\n    print(\"Cleaning text...\")\n    tqdm.pandas(desc=\"Cleaning texts\")\n    combined_df['clean_text'] = combined_df['text'].progress_apply(clean_text)\n\n    print(\"Filtering by length...\")\n    combined_df = combined_df[combined_df['clean_text'].apply(filter_by_length)]\n\n    print(\"Keeping only essential columns (source and clean_text)...\")\n    combined_df = combined_df[['source', 'clean_text']]\n\n    print(f\"Final dataset size: {len(combined_df)} documents\")\n    print(f\"Distribution by source:\")\n    source_distribution = combined_df['source'].value_counts()\n    for source, count in source_distribution.items():\n        print(f\"  {source}: {count} documents\")\n\n    return combined_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_dataset(df, output_path=\"dataset.csv\"):\n    \"\"\"Save the curated dataset to a CSV file.\"\"\"\n    df.to_csv(output_path, index=False)\n    print(f\"Dataset saved to {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collect_and_process_data():\n    print(\"Starting dataset collection...\")\n    datasets = collect_diverse_datasets()\n\n    print(\"Curating datasets...\")\n    curated_dataset = curate_dataset(datasets)\n\n    save_dataset(curated_dataset)\n    print(\"Dataset collection and curation complete!\")\n\n    return curated_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datasets = collect_diverse_datasets()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"curated_dataset = curate_dataset(datasets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dataset(curated_dataset, \"final_cleaned_dataset.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL FINE-TUNING AND TRAINING (GPT2)","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"my_dataset.csv\")\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 42\ntorch.manual_seed(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nMODEL_NAME = \"gpt2\"\nMAX_LENGTH = 512\nBATCH_SIZE = 8\nEPOCHS = 3\nLEARNING_RATE = 3e-5\nSAVE_DIRECTORY = \"./creative_content_model\"\nTRAIN_SIZE = 0.9\nGRADIENT_ACCUMULATION_STEPS = 2\nFP16_TRAINING = torch.cuda.is_available()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CreativeContentDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attention_masks = []\n        for text in tqdm(texts, desc=\"Tokenizing texts\"):\n            encodings = tokenizer(\n                text,\n                add_special_tokens=True,\n                max_length=max_length,\n                padding=\"max_length\",\n                truncation=True,\n                return_tensors=\"pt\"\n            )\n            self.input_ids.append(encodings[\"input_ids\"])\n            self.attention_masks.append(encodings[\"attention_mask\"])\n    def __len__(self):\n        return len(self.input_ids)\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx].squeeze(),\n            \"attention_mask\": self.attention_masks[idx].squeeze(),\n            \"labels\": self.input_ids[idx].squeeze()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_and_prepare_data(file_path, sample_size=None):\n    \"\"\"Load data from CSV and prepare for training\"\"\"\n    logger.info(f\"Loading data from {file_path}\")\n    df = pd.read_csv(file_path)\n\n    if sample_size and len(df) > sample_size:\n        df = df.sample(sample_size, random_state=SEED)\n\n    logger.info(f\"Dataset size: {len(df)} documents\")\n\n    source_types = df['source'].unique()\n    logger.info(f\"Source types: {source_types}\")\n\n    texts = df['clean_text'].tolist()\n\n    train_texts, val_texts = train_test_split(texts, train_size=TRAIN_SIZE, random_state=SEED)\n\n    logger.info(f\"Training samples: {len(train_texts)}\")\n    logger.info(f\"Validation samples: {len(val_texts)}\")\n\n    return train_texts, val_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model():\n    \"\"\"Train the language model with enhanced parameters for better output\"\"\"\n    logger.info(f\"Using model: {MODEL_NAME}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token  \n    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n     \n    train_texts, val_texts = load_and_prepare_data(\"my_dataset.csv\", sample_size=100000)  \n    \n    train_dataset = HFDataset.from_dict({\"text\": train_texts})\n    val_dataset = HFDataset.from_dict({\"text\": val_texts})\n   \n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    train_dataset = train_dataset.map(\n        tokenize_function,\n        batched=True,\n        desc=\"Tokenizing training data\",\n        remove_columns=[\"text\"],\n        num_proc=4  \n    )\n    val_dataset = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        desc=\"Tokenizing validation data\",\n        remove_columns=[\"text\"],\n        num_proc=4  \n    )\n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False  \n    )\n    \n    training_args = TrainingArguments(\n        output_dir=SAVE_DIRECTORY,\n        overwrite_output_dir=True,\n        num_train_epochs=EPOCHS,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  \n        eval_strategy=\"epoch\",    \n        save_strategy=\"epoch\", \n        save_total_limit=EPOCHS,  \n        learning_rate=LEARNING_RATE,\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",  \n        logging_dir=\"./logs\",\n        logging_strategy='epoch',\n        load_best_model_at_end=True,\n        metric_for_best_model=\"loss\",\n        greater_is_better=False,\n        fp16=FP16_TRAINING,  \n        dataloader_num_workers=4,  \n        group_by_length=True,  \n        report_to=\"tensorboard\"  \n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset\n    )\n    \n    trainer.processing_class = tokenizer\n    \n    logger.info(\"Starting training...\")\n    trainer.train()\n   \n    logger.info(f\"Saving model to {SAVE_DIRECTORY}\")\n    trainer.save_model(SAVE_DIRECTORY)\n    tokenizer.save_pretrained(SAVE_DIRECTORY)\n    return model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n  device = torch.device(\"cuda\")\n  logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n  print(device)\n    \n\nelse:\n  device = torch.device(\"cpu\")\n  logger.info(\"No GPU available, using CPU\")\n  print(\"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def download_model_to_local(model_directory=\"./creative_content_model\", \n                            output_zip=\"model_download.zip\"):\n    \"\"\"\n    Prepare model files for download by zipping them\n    \n    Parameters:\n    model_directory (str): Path to the saved model directory\n    output_zip (str): Filename for the zip file to create\n    \"\"\"\n    if not os.path.exists(model_directory):\n        raise FileNotFoundError(f\"Model directory {model_directory} not found\")\n    \n    print(f\"Compressing model files from {model_directory}...\")\n    \n    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(model_directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, os.path.dirname(model_directory))\n                zipf.write(file_path, arcname)\n    \n    print(f\"Model compressed successfully to {output_zip}\")\n    print(\"You can now download this file from the 'Output' tab in Kaggle\")\n\n    return output_zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = download_model_to_local()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GENERATE CREATIVE CONTENT","metadata":{}},{"cell_type":"code","source":"# Import dataset and model from saved kaggle inputs\n\ndf = pd.read_csv(\"/kaggle/input/curated-dataset/my_dataset.csv\")\n\nprint(\"Dataset loaded.\")\n\nmodel_path = \"/kaggle/input/gpt2-model/pytorch/default/1/creative_content_model\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nprint(f\"Model loaded and moved to {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_creative_content(model, tokenizer, prompt, max_length=200, num_return_sequences=3, temperature=0.9):\n    \"\"\"Generate creative content using the trained model with improved sentence completion.\"\"\"\n    model.eval()\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n\n    output = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        temperature=temperature,\n        top_k=50,\n        top_p=0.93,  \n        do_sample=True,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n        early_stopping=False,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.3  \n    )\n\n    import re\n    generated_texts = []\n    for ids in output:\n        text = tokenizer.decode(ids, skip_special_tokens=True)\n        text = re.sub(r'@-@', '', text)\n        text = re.sub(r'[^A-Za-z0-9\\s\\.\\,\\!\\?\\'\\\"\\;\\:\\-]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        text = ensure_complete_sentences(text)\n\n        generated_texts.append(text)\n\n    return generated_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ensure_complete_sentences(text):\n    \"\"\"Ensure the text ends with a complete sentence.\"\"\"\n    end_markers = ['.', '!', '?']\n    if text and text[-1] in end_markers:\n        return text\n    last_end = max([text.rfind(marker) for marker in end_markers])\n    if last_end != -1:\n        return text[:last_end+1]\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_generations(model, tokenizer, prompts=None):\n    \"\"\"Generate samples from different prompts.\"\"\"\n    if prompts is None:\n        prompts = [\n            \"The future of artificial intelligence looks\",\n            \"The most interesting aspect of creative writing is\",\n            \"The latest research on climate change suggests\"\n        ]\n\n    all_generated_texts = []\n\n    for prompt in prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        generated_texts = generate_creative_content(model, tokenizer, prompt)\n        all_generated_texts.append((prompt, generated_texts))\n\n        for i, text in enumerate(generated_texts):\n            print(f\"\\nGeneration {i+1}:\\n{text}\")\n        print(\"\\n\" + \"=\"*80)\n\n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_generations(model, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONTENT EVALUATION ","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    nlp = spacy.load(\"en_core_web_md\")\nexcept OSError:\n    print(\"Downloading spaCy model...\")\n    spacy.cli.download(\"en_core_web_md\")\n    nlp = spacy.load(\"en_core_web_md\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device='cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [\n    \"The future of artificial intelligence looks\",\n    \"The latest research on climate change suggests\",\n    \"If time travel were possible, humanity would\",\n    \"Deep beneath the ocean's surface, the greatest mystery awaiting explorers is\",\n    \"The beauty of creative writing lies in\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_fluency(text):\n    \"\"\"\n    Evaluate fluency by measuring:\n    1. Average sentence length\n    2. Lexical diversity (unique words / total words)\n    3. Grammatical correctness using a heuristic approach\n    \n    Args:\n        text (str): Text to evaluate\n        \n    Returns:\n        dict: Dictionary of fluency metrics\n    \"\"\"\n    if not text.strip():\n        return {\n            'avg_sent_length': 0,\n            'lexical_diversity': 0,\n            'grammatical_score': 0,\n            'fluency_score': 0\n        }\n        \n    sentences = sent_tokenize(text)\n    all_words = word_tokenize(text.lower())\n    words = [w for w in all_words if w.isalnum()]\n    \n    if len(sentences) > 0:\n        avg_sent_length = len(words) / len(sentences)\n    else:\n        avg_sent_length = 0\n        \n    if len(words) > 0:\n        lexical_diversity = len(set(words)) / len(words)\n    else:\n        lexical_diversity = 0\n        \n    \n    doc = nlp(text)\n    grammatical_score = 0\n    \n    for sent in doc.sents:\n        has_subj = any(token.dep_ in ('nsubj', 'nsubjpass') for token in sent)\n        has_verb = any(token.pos_ == 'VERB' for token in sent)\n        if has_subj and has_verb:\n            grammatical_score += 1\n    \n    if len(list(doc.sents)) > 0:\n        grammatical_score /= len(list(doc.sents))\n    \n    \n    fluency_score = (\n        min(avg_sent_length / 15, 1) * 0.3 +  \n        lexical_diversity * 0.3 +\n        grammatical_score * 0.4\n    )\n        \n    return {\n        'avg_sent_length': avg_sent_length,\n        'lexical_diversity': lexical_diversity,\n        'grammatical_score': grammatical_score,\n        'fluency_score': fluency_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_flexibility(text):\n    \"\"\"\n    Evaluate flexibility by analyzing:\n    1. Topic diversity\n    2. Semantic range\n    3. Concept switching\n    \"\"\"\n    if not text.strip():\n        return {\n            'topic_diversity': 0,\n            'semantic_range': 0,\n            'concept_transitions': 0,\n            'flexibility_score': 0\n        }\n        \n    doc = nlp(text)\n    key_nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN' and token.text.lower() not in stop_words]\n    topic_diversity = len(set(key_nouns)) / len(key_nouns) if key_nouns else 0\n        \n    sentences = list(doc.sents)\n    if len(sentences) >= 2:\n        sent_embeddings = np.array([sent.vector for sent in sentences])\n        similarities = cosine_similarity(sent_embeddings)\n        semantic_range = 1 - (np.sum(similarities) - len(sentences)) / (len(sentences) * (len(sentences) - 1))\n    else:\n        semantic_range = 0\n        \n    concept_transitions = 0\n    prev_key_entities = set()\n    for sent in sentences:\n        sent_entities = set([token.lemma_ for token in sent \n                             if token.pos_ in ('NOUN', 'PROPN') and token.text.lower() not in stop_words])\n        if prev_key_entities and (len(sent_entities.intersection(prev_key_entities)) / max(1, len(prev_key_entities)) < 0.3):\n            concept_transitions += 1\n        prev_key_entities = sent_entities\n    concept_transitions = concept_transitions / (len(sentences) - 1) if len(sentences) > 1 else 0\n        \n    flexibility_score = (\n        topic_diversity * 0.4 +\n        semantic_range * 0.4 +\n        concept_transitions * 0.2\n    )\n    \n    return {\n        'topic_diversity': topic_diversity,\n        'semantic_range': semantic_range,\n        'concept_transitions': concept_transitions,\n        'flexibility_score': flexibility_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_originality(text, reference_texts=None):\n    \"\"\"\n    Alternative evaluation of originality by:\n    1. Lexical novelty: proportion of unique trigrams in the text\n    2. Phrase novelty: proportion of trigrams not found in the reference corpus\n    3. Comparison to reference corpus via document vector similarity\n\n    This function avoids using a rare word frequency measure.\n    \"\"\"\n    if reference_texts is None:\n        reference_texts = []\n        \n    if not text.strip():\n        return {\n            'lexical_novelty': 0,\n            'phrase_novelty': 0,\n            'reference_similarity': 1,  \n            'originality_score': 0\n        }\n        \n    doc = nlp(text)\n    tokens = [token.text.lower() for token in doc if token.is_alpha]\n    \n    trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens) - 2)]\n    if not trigrams:\n        lexical_novelty = 0\n    else:\n        unique_trigrams = set(trigrams)\n        lexical_novelty = len(unique_trigrams) / len(trigrams)\n    \n    reference_trigrams = []\n    for ref_text in reference_texts:\n        ref_doc = nlp(ref_text)\n        ref_tokens = [token.text.lower() for token in ref_doc if token.is_alpha]\n        reference_trigrams.extend([' '.join(ref_tokens[i:i+3]) for i in range(len(ref_tokens) - 2)])\n    \n    phrase_novelty = (sum(1 for tg in trigrams if tg not in reference_trigrams) / len(trigrams)) if trigrams else 0\n        \n    doc_vector = doc.vector\n    reference_similarities = []\n    for ref_text in reference_texts:\n        ref_doc = nlp(ref_text)\n        similarity = cosine_similarity(\n            doc_vector.reshape(1, -1), \n            ref_doc.vector.reshape(1, -1)\n        )[0][0]\n        reference_similarities.append(similarity)\n    reference_similarity = max(reference_similarities) if reference_similarities else 0\n    \n    originality_score = (\n        lexical_novelty * 0.4 +\n        phrase_novelty * 0.3 +\n        (1 - reference_similarity) * 0.3\n    )\n    \n    return {\n        'lexical_novelty': lexical_novelty,\n        'phrase_novelty': phrase_novelty,\n        'reference_similarity': reference_similarity,\n        'originality_score': originality_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_reference_texts(query):\n    search_results = wikipedia.search(query)\n    reference_texts = []\n    for title in search_results[:3]:\n        try:\n            page = wikipedia.page(title)\n            reference_texts.append(page.content)\n            print(f\"Retrieved content for page: {title}\")\n        except Exception as e:\n            print(f\"Could not retrieve page for {title}: {e}\")\n    return reference_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_elaboration(text):\n    \"\"\"\n    Evaluate elaboration by analyzing:\n    1. Detail density\n    2. Descriptive richness (average adjectives per noun)\n    3. Explanation depth (using both keywords and dependency labels)\n    \"\"\"\n    if not text.strip():\n        return {\n            'detail_density': 0,\n            'descriptive_richness': 0,\n            'explanation_depth': 0,\n            'elaboration_score': 0\n        }\n    \n    doc = nlp(text)\n    tokens = list(doc)\n    \n    detail_tokens = [token for token in doc if token.pos_ in ('ADJ', 'ADV') or token.dep_ == 'prep']\n    detail_density = len(detail_tokens) / len(tokens) if tokens else 0\n    \n    nouns = [token for token in doc if token.pos_ in ('NOUN', 'PROPN')]\n    adjectives = [token for token in doc if token.pos_ == 'ADJ']\n    if nouns:\n        avg_adj_per_noun = len(adjectives) / len(nouns)\n    else:\n        avg_adj_per_noun = 0\n        \n    scaled_richness = min(avg_adj_per_noun / 0.5, 1)\n    \n    \n    explanation_keywords = {'because', 'since', 'therefore', 'thus', 'consequently', 'due', 'hence'}\n    keyword_count = sum(1 for token in doc if token.text.lower() in explanation_keywords)\n   \n    advcl_count = sum(1 for token in doc if token.dep_ == 'advcl')\n    total_explanation = keyword_count + advcl_count\n    sentences = list(doc.sents)\n    explanation_depth = total_explanation / len(sentences) if sentences else 0\n    \n    scaled_explanation = min(explanation_depth, 1)\n    \n    elaboration_score = (\n        detail_density * 0.4 +\n        scaled_richness * 0.3 +\n        scaled_explanation * 0.3\n    )\n    \n    return {\n        'detail_density': detail_density,\n        'descriptive_richness': scaled_richness,\n        'explanation_depth': scaled_explanation,\n        'elaboration_score': elaboration_score\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_all_dimensions(text, reference_texts=None):\n    fluency = evaluate_fluency(text)\n    flexibility = evaluate_flexibility(text)\n    originality = evaluate_originality(text, reference_texts)\n    elaboration = evaluate_elaboration(text)\n    \n    creativity_score = (\n        fluency['fluency_score'] * 0.25 +\n        flexibility['flexibility_score'] * 0.25 +\n        originality['originality_score'] * 0.25 +\n        elaboration['elaboration_score'] * 0.25\n    )\n    0\n    return {\n        'fluency': fluency['fluency_score'],\n        'flexibility': flexibility['flexibility_score'],\n        'originality': originality['originality_score'],\n        'elaboration': elaboration['elaboration_score'],\n        'creativity': creativity_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_scores = defaultdict(list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for prompt in prompts:\n    print(f\"\\nProcessing prompt: {prompt}\")\n    reference_texts = get_reference_texts(prompt)\n    \n    generated_texts = generate_creative_content(model, tokenizer, prompt, num_return_sequences=100)\n    \n    prompt_scores = defaultdict(list)\n    for i, generated_text in enumerate(generated_texts):\n        print(f\"\\nGenerated Text {i+1}:\\n{generated_text}\")\n        scores = evaluate_all_dimensions(generated_text, reference_texts)\n        \n        print(f\"\\nScores for sequence {i+1}:\")\n        for key, value in scores.items():\n            print(f\"{key.capitalize()} Score: {value:.3f}\")\n            all_scores[key].append(value)\n            prompt_scores[key].append(value)\n    \n    print(f\"\\nAverages for prompt: {prompt}\")\n    for key, values in prompt_scores.items():\n        print(f\"Average {key.capitalize()} Score: {np.mean(values):.3f}\")\n\nprint(\"\\nOverall Averages across all prompts:\")\nfor key, values in all_scores.items():\n    print(f\"Average {key.capitalize()} Score: {np.mean(values):.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndimensions = list(all_scores.keys())\ndata_for_boxplot = [all_scores[dim] for dim in dimensions]\n\nplt.boxplot(data_for_boxplot, labels=dimensions)\nplt.title(\"Distribution of Scores by Dimension\")\nplt.xlabel(\"Dimension\")\nplt.ylabel(\"Score\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\nfor dimension, scores in all_scores.items():\n    data = np.array(scores)\n\n    plt.hist(data, bins=20, density=True, alpha=0.5)\n    \n    density = gaussian_kde(data)\n    xs = np.linspace(min(data), max(data), 200)\n    plt.plot(xs, density(xs))\n\n    plt.title(f\"Density Plot for {dimension.capitalize()} Scores\")\n    plt.xlabel(\"Score\")\n    plt.ylabel(\"Density\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ADVANCED FINE-TUNING","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LoRALayer(nn.Module):\n    \"\"\"Low-Rank Adaptation layer wrapper\"\"\"\n    def __init__(self, base_layer, rank=8, alpha=16):\n        super().__init__()\n        self.base_layer = base_layer\n        self.in_features = base_layer.weight.shape[1]\n        self.out_features = base_layer.weight.shape[0]\n        \n        self.lora_A = nn.Parameter(torch.zeros((rank, self.in_features)))\n        self.lora_B = nn.Parameter(torch.zeros((self.out_features, rank)))\n        \n        nn.init.normal_(self.lora_A, std=0.02)\n        nn.init.zeros_(self.lora_B)\n        \n        self.alpha = alpha\n        self.rank = rank\n        \n        self.base_layer.weight.requires_grad = False\n        if hasattr(self.base_layer, 'bias') and self.base_layer.bias is not None:\n            self.base_layer.bias.requires_grad = False\n    \n    def forward(self, x):\n        base_output = self.base_layer(x)\n        lora_output = (x @ self.lora_A.T) @ self.lora_B.T\n        scaling = self.alpha / self.rank\n        \n        return base_output + scaling * lora_output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT2Dataset(Dataset):\n    \"\"\"Dataset for GPT-2 fine-tuning\"\"\"\n    def __init__(self, texts, tokenizer, max_length=512):\n        self.tokenizer = tokenizer\n        self.texts = texts\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        if isinstance(text, dict):\n            text = text.get('text', text.get('content', ''))\n            \n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'input_ids': encodings['input_ids'].squeeze(),\n            'attention_mask': encodings['attention_mask'].squeeze(),\n            'labels': encodings['input_ids'].squeeze()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdvancedFineTuner:\n    \"\"\"Combines Curriculum Learning, Dynamic Evaluation, and LoRA fine-tuning\"\"\"\n    def __init__(self, model_name=\"gpt2\", output_dir=\"./model_output\"):\n        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n        self.base_model = copy.deepcopy(self.model)  # Keep a copy of base model\n        \n        self.output_dir = output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        \n        self.lora_layers = {}\n        self.is_lora_applied = False\n        \n        self.adaptation_steps = 0\n        self.max_adaptation_steps = 500\n        self.dynamic_optimizer = None\n    \n        self.current_curriculum_stage = 0\n        \n        print(f\"Model initialized on {device}\")\n    \n    def apply_lora(self, rank=8, alpha=16, target_modules=None):\n        \"\"\"Apply LoRA to model layers\"\"\"\n        print(\"Applying LoRA to model...\")\n        \n        if target_modules is None:\n            target_modules = ['c_attn', 'c_proj', 'c_fc']\n        \n        for name, module in self.model.named_modules():\n            if any(target_name in name for target_name in target_modules):\n                if isinstance(module, nn.Linear):\n                    parent_name = name.rsplit('.', 1)[0]\n                    layer_name = name.split('.')[-1]\n                    \n                    lora_layer = LoRALayer(module, rank=rank, alpha=alpha)\n                    parent_module = self.model\n                    for part in parent_name.split('.'):\n                        parent_module = getattr(parent_module, part)\n                    \n                    setattr(parent_module, layer_name, lora_layer)\n                    \n                    self.lora_layers[name] = lora_layer\n        \n        self.is_lora_applied = True\n        return self\n    \n    def get_lora_params(self):\n        \"\"\"Get only LoRA parameters for optimization\"\"\"\n        if not self.is_lora_applied:\n            raise ValueError(\"LoRA has not been applied to the model yet\")\n        \n        params = []\n        for layer in self.lora_layers.values():\n            params.extend([layer.lora_A, layer.lora_B])\n        return params\n    \n    def prepare_curriculum(self, texts):\n        \"\"\"Prepare a curriculum of increasingly complex examples\"\"\"\n        print(\"Preparing curriculum...\")\n        \n        try:\n            nltk.data.find('tokenizers/punkt')\n            nltk.data.find('taggers/averaged_perceptron_tagger')\n        except LookupError:\n            nltk.download('punkt')\n            nltk.download('averaged_perceptron_tagger')\n        \n        complexity_scores = []\n        for entry in texts:\n            if isinstance(entry, dict):\n                text = entry.get('text', entry.get('content', ''))\n            else:\n                text = entry\n            \n            words = nltk.word_tokenize(text)\n            sentences = nltk.sent_tokenize(text)\n            \n            if not sentences:  \n                continue\n                \n            avg_sentence_length = len(words) / len(sentences) if sentences else 0\n            unique_words_ratio = len(set(words)) / len(words) if words else 0\n            long_words_ratio = sum(1 for w in words if len(w) > 6) / len(words) if words else 0\n            \n            pos_tags = nltk.pos_tag(words)\n            \n            subordinate_conj = sum(1 for _, tag in pos_tags if tag == 'IN')\n            adverbs = sum(1 for _, tag in pos_tags if tag.startswith('RB'))\n            \n            complexity = (\n                0.2 * min(1.0, avg_sentence_length / 25) +\n                0.2 * unique_words_ratio +\n                0.2 * long_words_ratio +\n                0.2 * min(1.0, subordinate_conj / 15) +\n                0.2 * min(1.0, adverbs / 10)\n            )\n            \n            complexity_scores.append({\n                'entry': entry,\n                'complexity': complexity,\n                'stats': {\n                    'avg_sentence_length': avg_sentence_length,\n                    'unique_words_ratio': unique_words_ratio\n                }\n            })\n        \n        complexity_scores.sort(key=lambda x: x['complexity'])\n        \n        num_entries = len(complexity_scores)\n        stage_size = num_entries // 5\n        \n        curriculum = []\n        for stage in range(5):\n            start_idx = stage * stage_size\n            end_idx = (stage + 1) * stage_size if stage < 4 else num_entries\n            \n            stage_entries = [item['entry'] for item in complexity_scores[start_idx:end_idx]]\n            avg_complexity = sum(item['complexity'] for item in complexity_scores[start_idx:end_idx]) / (end_idx - start_idx)\n            \n            curriculum.append({\n                'stage': stage + 1,\n                'entries': stage_entries,\n                'avg_complexity': avg_complexity\n            })\n            \n            print(f\"Stage {stage+1}: {len(stage_entries)} entries, avg complexity: {avg_complexity:.3f}\")\n        \n        return curriculum\n    \n    def setup_dynamic_evaluation(self, learning_rate=2e-5):\n        \"\"\"Setup dynamic evaluation components\"\"\"\n        self.adaptation_steps = 0\n        \n        if self.is_lora_applied:\n            params = self.get_lora_params()\n        else:\n            params = self.model.parameters()\n            \n        self.dynamic_optimizer = AdamW(params, lr=learning_rate)\n        return self\n    \n    def train(self, texts, training_args):\n        \"\"\"Main training method combining all three strategies\"\"\"\n        print(\"Starting advanced fine-tuning with Curriculum Learning, Dynamic Evaluation, and LoRA...\")\n        \n        if not self.is_lora_applied:\n            self.apply_lora(\n                rank=training_args.get('lora_rank', 8),\n                alpha=training_args.get('lora_alpha', 16)\n            )\n        \n        self.setup_dynamic_evaluation(\n            learning_rate=training_args.get('dynamic_lr', 2e-5)\n        )\n        \n        curriculum = self.prepare_curriculum(texts)\n        \n        batch_size = training_args.get('batch_size', 4)\n        curriculum_epochs = training_args.get('curriculum_epochs', [4, 3, 3, 2, 2]) \n        eval_interval = training_args.get('eval_interval', 100)\n        max_length = training_args.get('max_length', 512)\n        \n        optimizer = AdamW(\n            self.get_lora_params(),\n            lr=training_args.get('learning_rate', 5e-5)\n        )\n        \n        metrics_by_stage = []\n        \n        for stage_idx, stage in enumerate(curriculum):\n            print(f\"\\n===== Training on curriculum stage {stage['stage']} (complexity: {stage['avg_complexity']:.3f}) =====\")\n            self.current_curriculum_stage = stage['stage']\n           \n            stage_texts = stage['entries']\n            stage_dataset = GPT2Dataset(stage_texts, self.tokenizer, max_length=max_length)\n            \n            stage_dataloader = DataLoader(\n                stage_dataset,\n                batch_size=batch_size,\n                shuffle=True\n            )\n        \n            stage_epochs = curriculum_epochs[stage_idx] if stage_idx < len(curriculum_epochs) else 1\n            stage_metrics = []\n            \n            for epoch in range(stage_epochs):\n                self.model.train()\n                epoch_loss = 0\n                dynamic_updates = 0\n                \n                progress_bar = tqdm(stage_dataloader, desc=f\"Stage {stage['stage']}, Epoch {epoch+1}\")\n                for step, batch in enumerate(progress_bar):\n                    batch = {k: v.to(device) for k, v in batch.items()}\n                    \n                    outputs = self.model(**batch)\n                    loss = outputs.loss\n            \n                    loss.backward()\n                    optimizer.step()\n                    optimizer.zero_grad()\n                 \n                    if step % training_args.get('dynamic_update_interval', 10) == 0:\n            \n                        with torch.no_grad():\n                            sample_idx = random.randint(0, len(stage_texts) - 1)\n                            sample_text = stage_texts[sample_idx]\n                            if isinstance(sample_text, dict):\n                                sample_text = sample_text.get('text', sample_text.get('content', ''))\n                            \n                            prompt_ids = self.tokenizer.encode(sample_text[:100], return_tensors='pt').to(device)\n                            target_ids = self.tokenizer.encode(sample_text, return_tensors='pt').to(device)\n                            \n                            self.model.train()\n                            outputs = self.model(prompt_ids, labels=target_ids)\n                            dynamic_loss = outputs.loss\n  \n                            dynamic_loss_scaled = dynamic_loss * training_args.get('dynamic_weight', 0.3)\n                            dynamic_loss_scaled.backward()\n                            self.dynamic_optimizer.step()\n                            self.dynamic_optimizer.zero_grad()\n                            \n                            dynamic_updates += 1\n\n                    epoch_loss += loss.item()\n                    progress_bar.set_postfix({\n                        'loss': epoch_loss / (step + 1),\n                        'dynamic_updates': dynamic_updates\n                    })\n      \n                    self.adaptation_steps += 1\n                    if self.adaptation_steps >= self.max_adaptation_steps:\n                        print(\"Resetting model to prevent drift...\")\n                        \n                        with torch.no_grad():\n                            for (name1, param1), (name2, param2) in zip(\n                                self.model.named_parameters(), self.base_model.named_parameters()\n                            ):\n      \n                                if not self.is_lora_applied or name1 not in self.lora_layers:\n                                    # Pull 10% back toward base model\n                                    adjustment = 0.1 * (param2 - param1)\n                                    param1.add_(adjustment)\n                        \n                        self.adaptation_steps = 0\n         \n                avg_loss = epoch_loss / len(stage_dataloader)\n                print(f\"Stage {stage['stage']}, Epoch {epoch+1}: Average loss = {avg_loss:.4f}, Dynamic updates: {dynamic_updates}\")\n    \n                stage_metrics.append({\n                    'stage': stage['stage'],\n                    'epoch': epoch + 1,\n                    'avg_loss': avg_loss,\n                    'dynamic_updates': dynamic_updates\n                })\n  \n            metrics_by_stage.append({\n                'stage': stage['stage'],\n                'avg_complexity': stage['avg_complexity'],\n                'epochs': stage_epochs,\n                'metrics': stage_metrics\n            })\n \n            stage_dir = os.path.join(self.output_dir, f\"stage_{stage['stage']}\")\n            os.makedirs(stage_dir, exist_ok=True)\n\n            self.model.save_pretrained(stage_dir)\n\n            if self.is_lora_applied:\n                lora_state_dict = {\n                    name + '.lora_A': layer.lora_A,\n                    name + '.lora_B': layer.lora_B\n                    for name, layer in self.lora_layers.items()\n                }\n                torch.save(lora_state_dict, os.path.join(stage_dir, \"lora_weights.pt\"))\n    \n        final_dir = os.path.join(self.output_dir, \"final_model\")\n        os.makedirs(final_dir, exist_ok=True)\n        self.model.save_pretrained(final_dir)\n\n        import json\n        with open(os.path.join(self.output_dir, \"training_metrics.json\"), 'w') as f:\n            json.dump(metrics_by_stage, f, indent=2)\n        \n        print(\"Advanced fine-tuning completed successfully!\")\n        return self.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(self, prompt, max_length=100, temperature=0.8, use_dynamic_eval=True, **kwargs):\n        \"\"\"Generate text with the option to use dynamic evaluation\"\"\"\n        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(device)\n        \n        with torch.no_grad():\n            output_sequences = self.model.generate(\n                inputs,\n                max_length=max_length,\n                temperature=temperature,\n                top_k=kwargs.get('top_k', 50),\n                top_p=kwargs.get('top_p', 0.95),\n                do_sample=kwargs.get('do_sample', True),\n                num_return_sequences=kwargs.get('num_return_sequences', 1),\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        \n        generated_text = self.tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n \n        if use_dynamic_eval and self.dynamic_optimizer is not None:\n            target_ids = self.tokenizer.encode(generated_text, return_tensors='pt').to(device)\n            self.model.train()\n     \n            outputs = self.model(inputs, labels=target_ids)\n            loss = outputs.loss\n\n            scaled_loss = loss * 0.2\n\n            scaled_loss.backward()\n            self.dynamic_optimizer.step()\n            self.dynamic_optimizer.zero_grad()\n       \n            self.adaptation_steps += 1\n \n            if self.adaptation_steps >= self.max_adaptation_steps:\n                print(\"Resetting model to prevent drift...\")\n                with torch.no_grad():\n                    for (name1, param1), (name2, param2) in zip(\n                        self.model.named_parameters(), self.base_model.named_parameters()\n                    ):\n\n                        if not self.is_lora_applied or name1 not in self.lora_layers:\n                            adjustment = 0.1 * (param2 - param1)\n                            param1.add_(adjustment)\n                \n                self.adaptation_steps = 0\n        \n        return generated_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    fine_tuner = AdvancedFineTuner(\n        model_name=\"gpt2\",\n        output_dir=\"./advanced_gpt2_model\"\n    )\n    \n    texts = [\n        \"The future of artificial intelligence looks promising as researchers develop new algorithms for machine learning.\",\n        \"Recent scientific studies have shown that climate change is accelerating at an alarming rate due to human activity.\",\n        \"If time travel were possible, humanity would need to establish strict protocols to prevent paradoxes.\",\n        \"Deep beneath the ocean's surface, the greatest mystery awaiting explorers is the undiscovered ecosystem.\",\n        \"The beauty of creative writing lies in its ability to transport readers to different worlds and perspectives.\"\n    ]\n\n    texts = texts * 100  \n\n    training_args = {\n        'batch_size': 4,\n        'learning_rate': 5e-5,\n        'lora_rank': 8,\n        'lora_alpha': 16,\n        'curriculum_epochs': [4, 3, 3, 2, 2],\n        'dynamic_lr': 2e-5,\n        'dynamic_update_interval': 10,\n        'dynamic_weight': 0.3,\n        'max_length': 512\n    }\n    \n    fine_tuner.train(texts, training_args)\n    \n    prompts = [\n        \"The future of artificial intelligence looks\",\n        \"The latest research on climate change suggests\",\n        \"If time travel were possible, humanity would\",\n        \"Deep beneath the ocean's surface, the greatest mystery awaiting explorers is\",\n        \"The beauty of creative writing lies in\"\n    ]\n    \n    for prompt in prompts:\n        print(f\"\\nPrompt: {prompt}\")\n        \n        generated_text = fine_tuner.generate(\n            prompt, \n            max_length=200,\n            temperature=0.8,\n            use_dynamic_eval=True\n        )\n        \n        print(f\"Generated text: {generated_text}\")\n        print(\"-\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}