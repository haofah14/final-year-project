{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11071903,"sourceType":"datasetVersion","datasetId":6899743},{"sourceId":11085860,"sourceType":"datasetVersion","datasetId":6909618}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets accelerate torch wikipedia --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport transformers\nimport gc\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import Dataset\nfrom transformers import DataCollatorForLanguageModeling","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/my-dataset/my_dataset.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[[\"clean_text\"]].dropna()\n\ndf = df.sample(n=min(100000, len(df)), random_state=42).reset_index(drop=True)\n\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"EleutherAI/gpt-neo-125m\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"clean_text\"], padding=\"max_length\", truncation=True, max_length=216)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Tokenizing dataset...\")\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n\nmodel.gradient_checkpointing_enable()\n\ntraining_args = TrainingArguments(\n    output_dir=\"./llm_output\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,  \n    per_device_eval_batch_size=8,  \n    gradient_accumulation_steps=2, \n    num_train_epochs=3,  \n    learning_rate=3e-5,\n    fp16=True,  \n    logging_steps=2,  \n    save_total_limit=2,\n    push_to_hub=False,\n    report_to=\"none\",  \n    gradient_checkpointing=True,  \n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  \n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset.select(range(1000)),  \n    data_collator=data_collator\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training...\")\ntrainer.train()\n\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./llm_output\")\ntokenizer.save_pretrained(\"./llm_output\")\n\nprint(\"Training complete! Model saved to './llm_output'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef download_model_to_local(model_directory=\"./llm_output\", \n                            output_zip=\"gptneo_model_download.zip\"):\n    \n    if not os.path.exists(model_directory):\n        raise FileNotFoundError(f\"Model directory {model_directory} not found\")\n    \n    print(f\"Compressing model files from {model_directory}...\")\n    \n    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(model_directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, os.path.dirname(model_directory))\n                zipf.write(file_path, arcname)\n    \n    print(f\"Model compressed successfully to {output_zip}\")\n    print(\"You can now download this file from the 'Output' tab in Kaggle\")\n\n    return output_zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zip_path = download_model_to_local()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONTENT GENERATION","metadata":{}},{"cell_type":"code","source":"model_path = \"/kaggle/input/gpt-neo/llm_output\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\nprint(f\"Model loaded and moved to {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_creative_content(model, tokenizer, prompt, max_length=200, num_return_sequences=3, temperature=0.9):\n    \"\"\"Generate creative content using the trained model with improved sentence completion.\"\"\"\n    model.eval()\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n\n    output = model.generate(\n        input_ids,\n        attention_mask=attention_mask,\n        max_length=max_length,\n        num_return_sequences=num_return_sequences,\n        temperature=temperature,\n        top_k=50,\n        top_p=0.93,  \n        do_sample=True,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n        early_stopping=False,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.3  \n    )\n\n    import re\n    generated_texts = []\n    for ids in output:\n        text = tokenizer.decode(ids, skip_special_tokens=True)\n        text = re.sub(r'@-@', '', text)\n        text = re.sub(r'[^A-Za-z0-9\\s\\.\\,\\!\\?\\'\\\"\\;\\:\\-]', '', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        text = ensure_complete_sentences(text)\n\n        generated_texts.append(text)\n\n    return generated_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ensure_complete_sentences(text):\n    \"\"\"Ensure the text ends with a complete sentence.\"\"\"\n    end_markers = ['.', '!', '?']\n    if text and text[-1] in end_markers:\n        return text\n    last_end = max([text.rfind(marker) for marker in end_markers])\n    if last_end != -1:\n        return text[:last_end+1]\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONTENT EVALUATION","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nimport wikipedia\nfrom collections import defaultdict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    nlp = spacy.load(\"en_core_web_md\")\nexcept OSError:\n    print(\"Downloading spaCy model...\")\n    spacy.cli.download(\"en_core_web_md\")\n    nlp = spacy.load(\"en_core_web_md\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompts = [\n    \"The future of artificial intelligence looks\",\n    \"The latest research on climate change suggests\",\n    \"If time travel were possible, humanity would\",\n    \"Deep beneath the ocean's surface, the greatest mystery awaiting explorers is\",\n    \"The beauty of creative writing lies in\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_fluency(text):\n    \"\"\"\n    Evaluate fluency by measuring:\n    1. Average sentence length\n    2. Lexical diversity (unique words / total words)\n    3. Grammatical correctness using a heuristic approach\n    \n    Args:\n        text (str): Text to evaluate\n        \n    Returns:\n        dict: Dictionary of fluency metrics\n    \"\"\"\n    if not text.strip():\n        return {\n            'avg_sent_length': 0,\n            'lexical_diversity': 0,\n            'grammatical_score': 0,\n            'fluency_score': 0\n        }\n        \n    sentences = sent_tokenize(text)\n    all_words = word_tokenize(text.lower())\n    words = [w for w in all_words if w.isalnum()]\n    \n    if len(sentences) > 0:\n        avg_sent_length = len(words) / len(sentences)\n    else:\n        avg_sent_length = 0\n\n    if len(words) > 0:\n        lexical_diversity = len(set(words)) / len(words)\n    else:\n        lexical_diversity = 0\n\n    doc = nlp(text)\n    grammatical_score = 0\n\n    for sent in doc.sents:\n        has_subj = any(token.dep_ in ('nsubj', 'nsubjpass') for token in sent)\n        has_verb = any(token.pos_ == 'VERB' for token in sent)\n        if has_subj and has_verb:\n            grammatical_score += 1\n    \n    if len(list(doc.sents)) > 0:\n        grammatical_score /= len(list(doc.sents))\n        \n    fluency_score = (\n        min(avg_sent_length / 15, 1) * 0.3 +  \n        lexical_diversity * 0.3 +\n        grammatical_score * 0.4\n    )\n        \n    return {\n        'avg_sent_length': avg_sent_length,\n        'lexical_diversity': lexical_diversity,\n        'grammatical_score': grammatical_score,\n        'fluency_score': fluency_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_flexibility(text):\n    \"\"\"\n    Evaluate flexibility by analyzing:\n    1. Topic diversity\n    2. Semantic range\n    3. Concept switching\n    \"\"\"\n    if not text.strip():\n        return {\n            'topic_diversity': 0,\n            'semantic_range': 0,\n            'concept_transitions': 0,\n            'flexibility_score': 0\n        }\n        \n    doc = nlp(text)\n    key_nouns = [token.lemma_ for token in doc if token.pos_ == 'NOUN' and token.text.lower() not in stop_words]\n    topic_diversity = len(set(key_nouns)) / len(key_nouns) if key_nouns else 0\n        \n    sentences = list(doc.sents)\n    if len(sentences) >= 2:\n        sent_embeddings = np.array([sent.vector for sent in sentences])\n        similarities = cosine_similarity(sent_embeddings)\n        semantic_range = 1 - (np.sum(similarities) - len(sentences)) / (len(sentences) * (len(sentences) - 1))\n    else:\n        semantic_range = 0\n        \n    concept_transitions = 0\n    prev_key_entities = set()\n    for sent in sentences:\n        sent_entities = set([token.lemma_ for token in sent \n                             if token.pos_ in ('NOUN', 'PROPN') and token.text.lower() not in stop_words])\n        if prev_key_entities and (len(sent_entities.intersection(prev_key_entities)) / max(1, len(prev_key_entities)) < 0.3):\n            concept_transitions += 1\n        prev_key_entities = sent_entities\n    concept_transitions = concept_transitions / (len(sentences) - 1) if len(sentences) > 1 else 0\n        \n    flexibility_score = (\n        topic_diversity * 0.4 +\n        semantic_range * 0.4 +\n        concept_transitions * 0.2\n    )\n    \n    return {\n        'topic_diversity': topic_diversity,\n        'semantic_range': semantic_range,\n        'concept_transitions': concept_transitions,\n        'flexibility_score': flexibility_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_originality(text, reference_texts=None):\n    \"\"\"\n    Alternative evaluation of originality by:\n    1. Lexical novelty: proportion of unique trigrams in the text\n    2. Phrase novelty: proportion of trigrams not found in the reference corpus\n    3. Comparison to reference corpus via document vector similarity\n\n    This function avoids using a rare word frequency measure.\n    \"\"\"\n    if reference_texts is None:\n        reference_texts = []\n        \n    if not text.strip():\n        return {\n            'lexical_novelty': 0,\n            'phrase_novelty': 0,\n            'reference_similarity': 1,  \n            'originality_score': 0\n        }\n        \n    doc = nlp(text)\n    tokens = [token.text.lower() for token in doc if token.is_alpha]\n    \n    trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens) - 2)]\n    if not trigrams:\n        lexical_novelty = 0\n    else:\n        unique_trigrams = set(trigrams)\n        lexical_novelty = len(unique_trigrams) / len(trigrams)\n    \n    reference_trigrams = []\n    for ref_text in reference_texts:\n        ref_doc = nlp(ref_text)\n        ref_tokens = [token.text.lower() for token in ref_doc if token.is_alpha]\n        reference_trigrams.extend([' '.join(ref_tokens[i:i+3]) for i in range(len(ref_tokens) - 2)])\n    \n    phrase_novelty = (sum(1 for tg in trigrams if tg not in reference_trigrams) / len(trigrams)) if trigrams else 0\n        \n    doc_vector = doc.vector\n    reference_similarities = []\n    for ref_text in reference_texts:\n        ref_doc = nlp(ref_text)\n        similarity = cosine_similarity(\n            doc_vector.reshape(1, -1), \n            ref_doc.vector.reshape(1, -1)\n        )[0][0]\n        reference_similarities.append(similarity)\n    reference_similarity = max(reference_similarities) if reference_similarities else 0\n    \n    originality_score = (\n        lexical_novelty * 0.4 +\n        phrase_novelty * 0.3 +\n        (1 - reference_similarity) * 0.3\n    )\n    \n    return {\n        'lexical_novelty': lexical_novelty,\n        'phrase_novelty': phrase_novelty,\n        'reference_similarity': reference_similarity,\n        'originality_score': originality_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_reference_texts(query):\n    search_results = wikipedia.search(query)\n    reference_texts = []\n    for title in search_results[:3]:\n        try:\n            page = wikipedia.page(title)\n            reference_texts.append(page.content)\n            print(f\"Retrieved content for page: {title}\")\n        except Exception as e:\n            print(f\"Could not retrieve page for {title}: {e}\")\n    return reference_texts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_elaboration(text):\n    \"\"\"\n    Evaluate elaboration by analyzing:\n    1. Detail density\n    2. Descriptive richness (average adjectives per noun)\n    3. Explanation depth (using both keywords and dependency labels)\n    \"\"\"\n    if not text.strip():\n        return {\n            'detail_density': 0,\n            'descriptive_richness': 0,\n            'explanation_depth': 0,\n            'elaboration_score': 0\n        }\n    \n    doc = nlp(text)\n    tokens = list(doc)\n    \n    detail_tokens = [token for token in doc if token.pos_ in ('ADJ', 'ADV') or token.dep_ == 'prep']\n    detail_density = len(detail_tokens) / len(tokens) if tokens else 0\n    \n    nouns = [token for token in doc if token.pos_ in ('NOUN', 'PROPN')]\n    adjectives = [token for token in doc if token.pos_ == 'ADJ']\n    if nouns:\n        avg_adj_per_noun = len(adjectives) / len(nouns)\n    else:\n        avg_adj_per_noun = 0\n\n    scaled_richness = min(avg_adj_per_noun / 0.5, 1)\n    \n    explanation_keywords = {'because', 'since', 'therefore', 'thus', 'consequently', 'due', 'hence'}\n    keyword_count = sum(1 for token in doc if token.text.lower() in explanation_keywords)\n    \n    advcl_count = sum(1 for token in doc if token.dep_ == 'advcl')\n    total_explanation = keyword_count + advcl_count\n    sentences = list(doc.sents)\n    explanation_depth = total_explanation / len(sentences) if sentences else 0\n    \n    scaled_explanation = min(explanation_depth, 1)\n    \n    elaboration_score = (\n        detail_density * 0.4 +\n        scaled_richness * 0.3 +\n        scaled_explanation * 0.3\n    )\n    \n    return {\n        'detail_density': detail_density,\n        'descriptive_richness': scaled_richness,\n        'explanation_depth': scaled_explanation,\n        'elaboration_score': elaboration_score\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_all_dimensions(text, reference_texts=None):\n    fluency = evaluate_fluency(text)\n    flexibility = evaluate_flexibility(text)\n    originality = evaluate_originality(text, reference_texts)\n    elaboration = evaluate_elaboration(text)\n    \n    creativity_score = (\n        fluency['fluency_score'] * 0.25 +\n        flexibility['flexibility_score'] * 0.25 +\n        originality['originality_score'] * 0.25 +\n        elaboration['elaboration_score'] * 0.25\n    )\n    \n    return {\n        'fluency': fluency['fluency_score'],\n        'flexibility': flexibility['flexibility_score'],\n        'originality': originality['originality_score'],\n        'elaboration': elaboration['elaboration_score'],\n        'creativity': creativity_score\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_scores = defaultdict(list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for prompt in prompts:\n    print(f\"\\nProcessing prompt: {prompt}\")\n    reference_texts = get_reference_texts(prompt)\n    \n    generated_texts = generate_creative_content(model, tokenizer, prompt, num_return_sequences=100)\n    \n    prompt_scores = defaultdict(list)\n    for i, generated_text in enumerate(generated_texts):\n        print(f\"\\nGenerated Text {i+1}:\\n{generated_text}\")\n        scores = evaluate_all_dimensions(generated_text, reference_texts)\n        \n        print(f\"\\nScores for sequence {i+1}:\")\n        for key, value in scores.items():\n            print(f\"{key.capitalize()} Score: {value:.3f}\")\n            all_scores[key].append(value)\n            prompt_scores[key].append(value)\n    \n    print(f\"\\nAverages for prompt: {prompt}\")\n    for key, values in prompt_scores.items():\n        print(f\"Average {key.capitalize()} Score: {np.mean(values):.3f}\")\n\nprint(\"\\nOverall Averages across all prompts:\")\nfor key, values in all_scores.items():\n    print(f\"Average {key.capitalize()} Score: {np.mean(values):.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndimensions = list(all_scores.keys())\ndata_for_boxplot = [all_scores[dim] for dim in dimensions]\n\nplt.boxplot(data_for_boxplot, labels=dimensions)\nplt.title(\"Distribution of Scores by Dimension\")\nplt.xlabel(\"Dimension\")\nplt.ylabel(\"Score\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\nfor dimension, scores in all_scores.items():\n    data = np.array(scores)\n\n    plt.hist(data, bins=20, density=True, alpha=0.5)\n    \n    density = gaussian_kde(data)\n    xs = np.linspace(min(data), max(data), 200)\n    plt.plot(xs, density(xs))\n\n    plt.title(f\"Density Plot for {dimension.capitalize()} Scores\")\n    plt.xlabel(\"Score\")\n    plt.ylabel(\"Density\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}